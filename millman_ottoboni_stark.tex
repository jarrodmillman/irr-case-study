\documentclass[]{article}
\usepackage{geometry}                           % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                                  % ... or a4paper or a5paper or ... 
%\usepackage[parfill]{parskip}                  % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}                           % Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
                                                                % TeX will automatically convert eps --> pdf in pdflatex                
\usepackage{amssymb}
\usepackage{upquote}

%-----------------------------------------------------------------------------
% Special-purpose color definitions (dark enough to print OK in black and white)
\usepackage{color}
% A few colors to replace the defaults for certain link types
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}
%-----------------------------------------------------------------------------
% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{pdftex, % needed for pdflatex
  breaklinks=true, % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
}

%-----------------------------------------------------------------------------
%
% Commands for annotating the docs with fixme and inter-author notes.  See
% below for how to disable these.
%
% Define a \fixme command to mark visually things needing fixing in the draft,
% as well as similar commands for each author to leave initialed special
% comments in the document.
%% FIXME:  For final printing or to simply disable these bright warnings, copy
%% (there's a target macros_off' in the makefile that does this) the file
%% macros_off.tex to macros.tex

\newcommand{\fix}[1] { \textcolor{red} {
{\fbox{ {\bf Fix:} \ensuremath{\blacktriangleright }} {\bf #1}
\fbox{\ensuremath{\blacktriangleleft} } } } }

% And similarly, one (less jarring, with fewer symbols and no boldface) command
% for each one of us to leave comments in the main text.
\newcommand{\philip}[1] { \textcolor{blue} {
\ensuremath{\blacklozenge} {\bf philip:}  {#1}
\ensuremath{\blacklozenge} } }

\newcommand{\jarrod}[1] { \textcolor{darkgreen} {
\ensuremath{\bigstar} {\bf jarrod:}  {#1}
\ensuremath{\bigstar} } }

\newcommand{\kellie}[1] { \textcolor{darkorange} {
\ensuremath{\blacksquare} {\bf kellie:}  {#1}
\ensuremath{\blacksquare} } }

%% Uncomment these to turn all the special marker commands off
%\renewcommand{\fix}[1]{}
%\renewcommand{\philip}[1]{}
%\renewcommand{\jarrod}[1]{}
%\renewcommand{\kellie}[1]{}


\date{}

\begin{document}

\title{Reproducible Applied Statistics:\\
Is tagging of therapist-patient interactions reliable?\thanks{Submitted
to \url{https://github.com/BIDS/repro-case-studies}.
}}

\author{K. Jarrod Millman (JM)\\ Division of Biostatistics\\ UC Berkeley \and
Kellie Ottoboni (KO)\\ Department of Statistics\\ UC Berkeley \and
Naomi A. P. Stark (NS)\\ Department of Philosophy\\ University of Pennsylvania \and
Philip B. Stark (PS)\\ Department of Statistics\\ UC Berkeley
}


\maketitle


\section{Introduction}

This case study illustrates some of the reproducible practices we (JM, KO, PS)
have adopted when working with scientific or domain experts as applied
statisticians.  Parts of the text have been adapted from
\cite{millman2015thesis}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Who are you and what is your research field?
\end{enumerate}

%\begin{itemize}
%\item
%  K. Jarrod Millman, Division of Biostatistics, UC Berkeley PhD student
%  in biostatistics; background in scientific computing and neuroscience
%\item
%  Kellie Ottoboni, Department of Statistics, UC Berkeley PhD student in
%  statistics
%\item
%  Naomi A.P. Stark, Department of Philosophy, University of Pennsylvania
%  Undergraduate in bioethics; background in therapy with children on the
%  autistic spectrum
%\item
%  Philip B. Stark, Department of Statistics, UC Berkeley Faculty in
%  statistics; background in physical science
%\end{itemize}

We are three applied statisticians~(JM,~KO,~PS) at Berkeley working with a
domain specialist~(NP) at the University of Pennsylvania. Our case study
involves assessing inter-rater reliability (IRR) for human classifiers of
therapy sessions with children on the autistic spectrum.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Define what the term ``reproducibility'' means to you generally and/or
  in the particular context of your case study.
\end{enumerate}

In the context of the current project, \emph{reproducibility} means that
we (JM, KO, PS) have documented (nearly) every step of the analysis,
from cleaning to coding to code execution.
By keeping all code, text, and data in a publicly accessible version
controlled repository, we've made this well-documented analysis available
for anyone to examine.
While we've not made the original raw data available, we have made the
anonymized version public.
We have released both the original anonymized version as well as our cleaned
version including the exact commands necessary to produce the cleaned
version from the anonymized one.
Since we used GitHub's pull request mechanism to review every step of the
process, we've also made much of our internal design discussions visible
and not just the results of those discussions.
In addition to making what we did transparent to anyone who is interested,
working in this way means that when errors are found we can identify
exactly how and when those errors were introduced.
We also have written tests for (almost) all our code, which means we
can have a high level of confidence that as we make changes to our
codebase we can catch any cascading errors that result.
This means we can quickly and easily identify the causes of error, fix
those errors, and rerun the entire analysis from start to finish.

\section{Workflow narrative}\label{workflow-narrative}

\philip{
Over the 2014-2015 winter break, NS posed PS a question that came up in her
work on therapeutic interventions with children on the autism scale.
After coming to an initial understanding of her statistical problem,
PS sent JM and KO a one page proposal for a stratified permutation test for
multi-rater inter-rater reliability.
}

After (coming to an initial) understanding of NP's underlying research problem
and making certain we (JM, KO, PS) understood how her data was collected, we
cleaned the data, developed a nonparametric approach to assessing IRR
appropriate to the experiment, implemented the approach in Python, incorporated
the resulting code into an evolving Python package of permutation tests,
applied the approach to the cleaned data, documented the code and the analysis,
and wrote up the results in a \LaTeX document. 

We distinguish the following aspects of our study:
(1) understand problem,
(2) get and clean data,
(3) design algorithm,
(4) implement algorithm,
(5) analyze data, and
(6) understand result.
In Figure~\ref{fig:work_process}, we've diagrammed how each aspect of the
project influenced the other aspects as well as an estimate of how much time
we collectively spent on each aspect of the project.
Our estimates are based on the total person hours we spent on each aspect of
the problem.
For example, if all three of us (JM, KO, PS) spent an hour discussing the
problem in a meeting, then that hour involved 3 people hours.
Below we describe the work involved in each aspect of the project.
We've listed the total number of people hours spent on each aspect in the
headings for each subsection.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.8\textwidth]{work_process.png}
  \caption{
  \small
    Each box corresponds to one aspect of our project.
    The percents are estimates of the percent of our time spent on each aspect.
    Edges represent influence.
    \texttt{Understand problem}~$\to$~\texttt{Design algorithm}, for instance,
    corresponds to the fact that we first had to understand the problem
    in order to design the correct statistical test.\label{fig:work_process}}
\end{figure}

Underlying all aspects of this study is a set of computational practices we
(JM, KO, PS) followed (almost) whenever we were working on the project.
These computational practices are described in detail here
\cite{millman2014developing} and are used widely in the open source scientific
Python community.
Following these practices, we've developed a Python package for permutation
tests and confidence sets called
\texttt{permute}.\footnote{\url{http://statlab.github.io/permute/}}
While developed for managing software contributions, these practices are ideal
for ensuring computational reproducibility in scientific and statistical
research.
We will illustrate how we leverage the software infrastructure and development
practices of \texttt{permute} to conduct reproducible and collaborative applied
statistics research with our colleagues.
We discuss the software tools and practices briefly in \S~\ref{key-tools} below.

\subsection{Understand problem (80 hours)}

\philip{
NP was interested in:  What happens in therapy session with autistic kids?
In particular, NP had data and wanted to know whether humans can reliably rate
therapist-patient interactions?
conversations between NS and (primarily) PS to understand the experimental
set-up.
%Time: approximately 10 1-hour meetings
}

Once PS had an initial understanding of NS's problem, we (JM, KO, PS)
met regularly (approximately weekly, sometimes more) as a team to discuss the
project.  Initially these discussions involved a lot of work on whiteboards
and asking a lot of probing and open-ended questions.  This helped us
make sure we all gained a shared understanding of the problem.  It also
meant that we could improve our understanding by explaining it to one
another as well as asking hard questions about our planned approach
and whether it could address the question of interest.
As our understanding of the problem progressed, our work transitioned
from mostly working on whiteboards to testing our ideas out on a
computer.  Often we often used pair programming at this stage and sometimes
all three of us (JM, KO, PS) sat in front of one computer, while one of us
would type code in an interactive IPython session.
This helped ensure that we all understood the problem well and it also
helped us catch errors (typos as well as conceptual errors).
%Time: approximately 10 2-hour meetings.

\subsection{Get and clean data (13 hours)}

The data were collected by NS and Gilbert Kliman of the Children's
Psychological Health Center in San Francisco.
They comprise ratings of segments of 8 videos by 10 trained raters.
Each video is divided into approximately 40 time segments.
In each time segment, none, any, or all of 183 types of activity might be
taking place.
The raters indicated which of those activities was taking place during each
segment of each video.

\philip{
receipt of preliminary data as an Excel spreadsheet; understanding the ``data
dictionary''; vetting for obvious errors (PS);
%Time: approximately 1 hour
several ``trips to the well'' before getting a version of the data that did not
have obvious errors (PS);
%Time: approximately 4 hours
export the Excel data to .csv format (PS);
%Time: approximately 5 minutes.
data anonymization: substitute unique numerical identifiers for raters' names. 
This step was performed using regular expressions in an interactive text
editor, on the .csv file.
It was not performed reproducibly (i.e., not scripted), but it can be checked
readily. (PS) 
%Time: approximately 1 hour.
}

After PS generated the original anonymized data, we committed it to our
repository and added a data loader with tests to ensure that if the data
changed we would know.
At this point, we (JM, PS) screened the anonymized data for transcription
errors (e.g., typos, duplications).  This involved writing a number of EDA
tools, which are now included in \texttt{permute}, to detect (for example)
duplicate rows as well as duplicate consecutive rows.
%Time: approximately 5 hours
Once we identified entries incompatible with our understanding of what
should be in the data, JM developed \texttt{sed} and \texttt{awk} scripts
to ``correct'' the data for duplicated entries and inferred typos.
%Time: included above
The exact commands used to clean the data are included in the commit
messages corresponding to that cleaning step.
After carefully examining the data for potential errors and documenting
every change we made and why, we sent the cleaned data and an explanation
of what we did to NS to verify that the corrections were appropriate.
%Time: 1 hour
As a result, we provide the cleaned data in our project repository as well
as a careful account of its provenance.

\subsection{Design algorithm (25 hours)}

Although we eventually implemented something close to the original test
proposed by PS at the start of the project, we spent significant time
focused on  ``problem appreciation.''  These conversations culminated
in our (JM, KO, PS) decision to assess the reliability one category at a time.
%Time: approximately 4h.
We also developed an understanding that it was best to treat the videos
separately.  A literature search for extant approaches to assessing IRR
led us to conclude that there was no existing suitable method, in part because
the experiment was stratified and in part because standard methods make
indefensible parametric assumptions, which we hoped to avoid.
%Time: 3h
After deciding to use permutation tests, we (JM, KO, PS) then determined what
the appropriate permutation would be: permuting each rater's ratings within a
video, independently across raters and across videos.
We chose to use concordance of ratings as our test statistic within each
stratum.  Finally, we (JM, PS) derived a simple expression for computing the
concordance efficiently.
%Time: 1h
To combine tests across strata, we (JM, KO, PS) decided to use the nonparametric
combination (NPC) of tests.
%Time: 1h
Finally, we (JM, KO, PS) developed a computationally efficient approach to
finding the overall p-value for NPC.
%Time: 1.5h

\subsection{Implementing the algorithm (5 hours)}

\jarrod{I plan to go over this:  https://github.com/statlab/permute/issues/69.
I will write this up as a little report separate from this case study.  I will
also look into how long it takes to run, but I am thinking I might make a
separate repository just for this.  I suspect it will take some time to run and
our tests and doc generation are already costly.  I am also reluctant to put
something we plan to freeze in a project that should evolve.  I am imagining
something that depends on the exact release of permute that we used and has a
permanent web address.}

\jarrod{Once I finish I need to discuss this with Kellie, since she did most of
the hard work as I recall.}
  

\subsection{Analyze data (1 hour)}

By the time we got to this stage, the analysis of the cleaned data was only
about 70 lines of Python, of which 56 are code (the rest are comments).
This includes looping over the 183 categories of activity.

\jarrod{Once I turn the analysis into a stand-alone frozen project, I
will report our results here.}

\subsection{Understand result (20 hours)}

\philip{You will probably want to wait until I have written up the results to
work on this; but, eventually we need a discussion of our final thoughts about
the data and how it did or did not address the question.  Maybe a sentence or
two saying what we learned...what we do differently during data collection next
time.  Understanding result can lead to 1) designing new experiment, 2)
collecting new data, 3) performing different analysis, 4) writing manuscript,
5) something else.  Need to connect this back to the our understanding of the
problem.  We should also say that despite not pursuing this further, it did
yield a new permutation test that could be used in other contexts and which is
available in \texttt{permute}.  }

\section{Pain points}\label{pain-points}

%\emph{Describe in detail the steps of a reproducible workflow which you
%consider to be particularly painful. How do you handle these? How do you
%avoid them? (200-400 words)}

The steps of our reproducible practices that we found particularly painful or
difficult varied depending on how familiar we were with the tools and practices
before the beginning of the project.

Since JM had been using these tools and practices for several years whenever
working with data and code on a computer, this aspect of the project was
pleasant and familiar.
So (for JM) the most painful part of the project was vetting hand-entered data
to ensure the data were relatively error-free.
Not only was this a laborious process, but it involved inferring what the data
should have been without any way to ensure that these inferences were correct.
The solution to this pain point is to automate the data collection process as
much as possible.
However, when data has already been entered by hand, there is not much that can
be done other than being cautious when "fixing" data entry errors and recording
every aspect of the data cleaning process.

For KO and PS there was a learning curve to master the tools and practices.
This involved understanding the data model used by Git, acquiring habits such
as writing tests for all functions and following a common style guide, as well
as learning to contribute to the project repository indirectly through GitHub's
pull request mechanism.
While learning how to use these tools and acquiring these habits takes effort
and persistence, once mastered the difficulties disappear and the benefits
accrue.

\section{Key benefits}\label{key-benefits}

%\emph{Discuss one or several sections of your workflow that you feel
%makes your approach better than the ``normal'' non-reproducible workflow
%that others might use in your field. What does your workflow do better
%than the one used by your lesser-skilled colleagues and students, and
%why? What would you want them to learn from your example? (200-400
%words)}

Since \cite{buckheit1995wavelab} popularized the idea of computational
reproducibility, many applied statisticians have taken the ideas of version
control and process automation seriously.
At Berkeley, many faculty and researchers in the Department of Statistics and
in the Division of Biostatistics have made the idea of computational
reproducibility central in both their classroom as well as their own work.
Several faculty require that anyone working with them use version
control.\footnote{For example, see:
\url{http://www.stat.berkeley.edu/~epurdom/studentResources.html}}

However, we believe that the practices we adopted go beyond the standard
operating practice of many of our colleagues.
In particular, our adherence to continuous integration and code review is
unusual among our colleagues.
We believe that our colleagues would have more reliable code and would be more
confident in their ability to modify their existing code without breaking
downstream dependencies by adopting these practices.

In particular, following our work practice has the following benefits:
\begin{enumerate}
\item It's easy to modify the analysis if errors are found, to apply the
  analysis to new data sets, and so on.
\item The process is largely self-documenting, making it easier to draft a
  paper about the results.
\item The methods are abstracted from the analysis and incorporated into a
  package so that others can discover, check, use, and extend the methods.
\end{enumerate}

\section{Key tools and practices}\label{key-tools}

%\emph{If applicable, provide a detailed description of a particular
%specialized tool that plays a key role in making your workflow
%reproducible, if you think that the tool might be of broader interest or
%relevance to a general audience. (200-400 words)}

As part of the development of our software package \texttt{permute}, we
invested significant effort in setting up a development infrastructure to
ensure our work is tracked, thoroughly and continually tested, and
incrementally improved and documented.
To this end, we have adopted best practices for software development used by
many successful open source projects \cite{millman2014developing}.

\subsection{\label{sec:vc}Version control and code review}

We (JM, KO, PS) use Git\footnote{\url{http://git-scm.com}} as our version
control system (VCS) and GitHub\footnote{\url{https://github.com}} as the
public hosting service for our official \texttt{upstream} repository
(\url{https://github.com/statlab/permute}).
Each of us has our own copy, or fork, of the \texttt{upstream} repository.
We each work on our own repositories and use the \texttt{upstream} repository
as our coordination or integration repository.

This allows us to track and manage how our code changes over time as well as
review all new functionality before merging it into the \texttt{upstream}
repository.
To get new code or text integrated in the \texttt{upstream} repository, we use
GitHub's \emph{pull request} mechanism.
This enables us to review code and text before integrating it.
Below, we describe how we automate testing our code to generate reports for all
pull requests.
This way we can reduce the risk that changes to our code break existing
functionality.
Once a pull request is reviewed and accepted, it is merged into the
\texttt{upstream} repository.

Requiring all new code to undergo review provides several benefits.
Code review increases the quality and consistency of our codebase.
It helps maintain a high level of test coverage (see below).
Moreover, it also helps keep the development team aware of the work other team
members are doing.
While we are currently a small team and we meet regularly, having the code
review system in place will make it easier for new people to contribute as well
as capturing our design discussions and decisions for future reference.

\subsection{\label{sec:test}Testing and continuous integration}

We use the \texttt{nose} testing framework for automating our testing
procedures.\footnote{\url{https://nose.readthedocs.org}}
This is the standard testing framework\footnote{As \texttt{nose} has been in
maintenance mode for the last few years, several projects are starting to look
into newer testing frameworks.
We are migrating to \texttt{pytest}: \url{http://pytest.org}.
While we may transition to a new testing framework, any system we
migrate to will have the same basic features and benefits as \texttt{nose}.}
used by the core packages in the scientific Python ecosystem.
Automating the tests allows us to monitor a proxy for code correctness when
making changes as well as simplifying the code review process for new code.
Without automated testing, we would have to manually test all the code every
time a change is proposed.
The \texttt{nose} testing framework simplifies test creation, discovery, and
running.
It has an extensive set of plugins to add functionality for coverage reporting,
test annotation, profiling, as well as inspecting and testing documentation.

\begin{figure}
  \begin{centering}
    \includegraphics[width=\textwidth]{pull-request-ci.png}\par
  \end{centering}

  \caption{\label{fig:pull-request}
  \small
    Each pull request triggers an automated system to run the  full test suite on
    the updated codebase.
    This means that when you go to review a pull request you can immediately see
    whether the change breaks any of the tests as well as whether the new
    code decreases the overall test coverage.
    For example, the above report indicates that the associated pull request does not
    break existing code and does not change our test coverage.}
\end{figure}

Our goal is to test every line of code.
For example, not only do we want to test every function in our package, but if
a specific function has internal logic we want to test each possible execution
path through the function.
Having tested each line of code increases our confidence in our codebase, but
more importantly provides us some measure of assurance that changes we make do
not break existing code.
It also increases our confidence that new code works, which reduces the
friction of accepting contributions.
Currently over 98\% of \texttt{permute}'s lines of code get executed at least
once by our test system.

We have configured Travis CI\footnote{\url{https://travis-ci.org}} and
\texttt{coveralls}\footnote{\url{https://coveralls.io}} to be automatically
triggered whenever a commit is made to a pull request or the upstream master
(see Figure~\ref{fig:pull-request}).
These systems then run the full test suite  using different versions of our
dependencies (e.g., Python 2.7 and 3.4) every time a new commit is made to a
repository or pull request.

\subsection{\label{sec:doc}Documentation}

We use Sphinx\footnote{\url{http://sphinx-doc.org}} as our documentation system
and already have good developer documentation and the foundation for
high-quality user documentation.
Sphinx is the standard documentation system for Python projects and is used by
the core scientific Python packages.
We use Python docstrings and follow the NumPy docstring
standard\footnote{\url{https://github.com/numpy/numpy/blob/master/doc/HOWTO\_DOCUMENT.rst.txt}}
to document all the modules and functions in \texttt{permute}.
Using Sphinx and some NumPy extensions, we have a system for autogenerating the
project documentation (as HTML or PDF) using the docstrings as well as
stand-alone text written in a light-weight markdown-like language, called
reStructuredText\footnote{\url{http://docutils.sourceforge.net/rst.html}}.
This system enables us to easily embed references, figures, code that is
auto-run during documentation generation, as well as mathematics using \LaTeX.

\subsection{\label{sec:release}Release management}

Our development workflow ensures that the official \texttt{upstream} repository
is always stable and ready for use.
This means anyone can get our official upstream master at any point, install it
and start using it.
We also make official releases available as source tarballs as well as Python
built-packages\footnote{Presently our code is pure Python, but we release
Python wheels.
Wheels are the new standard built-package format for Python.} uploaded to the
Python Package Index, or PyPI,\footnote{PyPI is the Python equivalent of \emph{The
Comprehensive R Archive Network} (CRAN).} with release announcements posted to
our mailing list.

To install the latest release of \texttt{permute} and its dependencies, type
the following command from a shell prompt (assuming you have Python and a
recent version of pip):

\texttt{\$ pip install permute}


%\textbf{Software tools:}
%
%    \begin{enumerate}
%    \def\labelenumiii{\alph{enumiii}.}
%    \itemsep1pt\parskip0pt\parsep0pt
%    \item
%      \emph{virtualenv} to control which versions of which software
%      packages were used. \emph{virtualenv} is a tool to create isolated
%      Python environments.
%    \item
%      \emph{git} for revision control
%    \item
%      \emph{github} for hosting
%    \item
%      \emph{make} for process automation
%    \item
%      \emph{nose} to automate and document testing
%    \item
%      \emph{sphinx} for documentation in HTML and PDF
%    \item
%      \emph{Python} as the language for data analysis
%    \item
%      \emph{Github} pull requests to control code review.
%    \item
%      \emph{TravisCI} and \emph{BuildBot} for continuous integration
%      during code development
%    \item
%      \emph{Coverage/Coveralls} to check the code coverage of the
%      \emph{nose} tests
%    \item
%      \emph{PyPI} to distribute built packages
%    \item
%      LaTeX for documents, especially documents containing mathematical
%      notation
%    \end{enumerate}
%    
%\textbf{Coding practices.}
%
%    \begin{enumerate}
%    \def\labelenumiii{\alph{enumiii}.}
%    \itemsep1pt\parskip0pt\parsep0pt
%    \item
%      Tests for all code; Coverage/Coveralls to check test coverage.
%    \item
%      Pair programming, at least some of the time
%    \item
%      Issue trackers in git
%    \item
%      All code vetted using pull requests: no pushes. Circular flow of
%      code from main repo to individual repos to main repo.
%    \item
%      Use whiteboard to sketch algorithms before coding
%    \item
%      Documentation: internal to the code, external in the Github repo
%      and github.io; external in the package
%    \item
%      Test data included with the package
%    \end{enumerate}


\bibliographystyle{plain}
\bibliography{millman_ottoboni_stark}

\end{document}
