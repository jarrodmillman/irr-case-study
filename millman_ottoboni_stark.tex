\documentclass[]{article}
\usepackage[round]{natbib}
\usepackage{geometry}               % See geometry.pdf to learn layout options
\geometry{letterpaper}              % ... or a4paper or a5paper or ... 
%\usepackage[parfill]{parskip}      % Activate to begin paragraphs with an
                                    %   empty line rather than an indent
\usepackage{graphicx}               % Use pdf, png, jpg, or eps with pdflatex;
                                    %   use eps in DVI mode;
                                    %   TeX automatically converts
                                    %     eps --> pdf in pdflatex                
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{upquote}

%-----------------------------------------------------------------------------
% Special-purpose color definitions (dark enough to print OK in black and white)
\usepackage{color}
% A few colors to replace the defaults for certain link types
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}
%-----------------------------------------------------------------------------
% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{pdftex, % needed for pdflatex
  breaklinks=true, % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
}

%-----------------------------------------------------------------------------
%
% Commands for annotating the docs with fixme and inter-author notes.  See
% below for how to disable these.
%
% Define a \fixme command to mark visually things needing fixing in the draft,
% as well as similar commands for each author to leave initialed special
% comments in the document.
% For final printing or to simply disable these bright warnings
% by uncommenting lines below "to turn all the special marker commands off"

\newcommand{\fix}[1] { \textcolor{red} {
{\fbox{ {\bf Fix:} \ensuremath{\blacktriangleright }} {\bf #1}
\fbox{\ensuremath{\blacktriangleleft} } } } }

% And similarly, one (less jarring, with fewer symbols and no boldface) command
% for each one of us to leave comments in the main text.
\newcommand{\philip}[1] { \textcolor{blue} {
\ensuremath{\blacklozenge} {\bf philip:}  {#1}
\ensuremath{\blacklozenge} } }

\newcommand{\jarrod}[1] { \textcolor{darkgreen} {
\ensuremath{\bigstar} {\bf jarrod:}  {#1}
\ensuremath{\bigstar} } }

\newcommand{\kellie}[1] { \textcolor{darkorange} {
\ensuremath{\blacksquare} {\bf kellie:}  {#1}
\ensuremath{\blacksquare} } }

%% Uncomment these to turn all the special marker commands off
%\renewcommand{\fix}[1]{}
%\renewcommand{\philip}[1]{}
%\renewcommand{\jarrod}[1]{}
%\renewcommand{\kellie}[1]{}


\date{}

\begin{document}

\title{A case study in reproducible applied statistics:\\
Is tagging of therapist-patient interactions reliable?
}

\author{K. Jarrod Millman (JM)\\ Division of Biostatistics\\ UC Berkeley \and
Kellie Ottoboni (KO)\\ Department of Statistics\\ UC Berkeley \and
Naomi A. P. Stark (NS)\\ Department of Philosophy\\ University of Pennsylvania \and
Philip B. Stark (PS)\\ Department of Statistics\\ UC Berkeley
}

\maketitle


\section{Introduction}

This case study\footnote{This file along with the analysis script and results
can be found here: \url{https://github.com/statlab/nsgk}} illustrates some of
the reproducible practices we (JM, KO, PS) have adopted.
Parts of the text have been adapted from \citet{millman2015thesis} and a
shorter version was submitted to
\url{https://github.com/BIDS/repro-case-studies}.


\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Who are you and what is your research field?
\end{enumerate}

We are three applied statisticians~(JM,~KO,~PS) at Berkeley working with a
domain specialist~(NS) at the University of Pennsylvania.
Our case study involves assessing inter-rater reliability (IRR) for human
classifiers of therapy sessions with children on the autistic spectrum.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Define what the term ``reproducibility'' means to you generally and/or
  in the particular context of your case study.
\end{enumerate}

In this case study, \emph{reproducibility} means that we (JM, KO, PS) have
documented nearly every step of the analysis---from cleaning to coding to
code execution---as well as much of the discussion preceding our decision
to take those steps.
It also means that we have thoroughly and in an automated fashion tested
our code so that we are more justified in our confidence that it is
correct.
Finally, it also means that we invested time to understand the fundamental
problem and the results of our analysis so that we do \emph{not} make
conclusions, which are not justified by the data, the manner in which it was
acquired, and our domain understanding.

By keeping all code, text, and data in a publicly available version
controlled repository, we have made our well-documented analysis available
for anyone to examine.
We have released the data used in our study---both the original anonymized
version as well as our cleaned version including the exact commands necessary
to produce the cleaned version from the anonymized one.

In addition to making what we did transparent to anyone who is interested,
working in this way means that when errors are found we can identify
exactly how and when those errors were introduced.
We have written tests for almost all our code, which means we have a high
level of confidence that as we change our code we can catch any cascading
errors that result.
Catching errors early means we quickly and easily identify our mistakes and
correct them.
And since we have automated the process of running our analysis, once errors
are identified and corrected, it is trivial to rerun the entire analysis from
start to finish.

Assuming you have network access and that you have a few standard tools (see
\S~\ref{key-tools}) for scientific computing installed on your computer, you
can run our complete analysis of the cleaned data by typing the following three
commands from a shell prompt:
\begin{verbatim}
$ git clone git@github.com:statlab/nsgk.git
$ cd nsgk/nsgk
$ make
\end{verbatim}
The first command creates a directory named \texttt{nsgk} in your current
working directory with a copy of the project repository (i.e., a directory with
our code, data, and text along with the provenance of these documents).
This directory contains this document as well as everything needed to run our
analysis.
Inside of \texttt{nsgk/nsgk} there is a \texttt{Makefile}, our analysis script
\texttt{analysis.py}, and the output \texttt{results.csv} of that script.

When you enter the command \texttt{make}, the following commands will be run:
\begin{verbatim}
virtualenv -p /usr/bin/python2.7 venv
venv/bin/pip install --upgrade pip
venv/bin/pip install -r requirements.txt
venv/bin/python analysis.py
\end{verbatim}
The first command creates a new virtual environment (\texttt{venv}) for Python
2.7.
Using this new virtual environment (\texttt{venv}) the subsequent commands
respectively upgrade the Python package manager (\texttt{pip}) to the most
recent version, install the necessary Python package dependencies
(\texttt{numpy}, \texttt{scipy}, and \texttt{permute}), and run the analysis
script \texttt{analysis.py}.

\section{Workflow narrative}\label{workflow-narrative}

Our project arose from a pilot study NS was working on with Gilbert Kliman of
the Children's Psychological Health Center in San Francisco.
While investigating therapeutic interventions in children on the autism scale,
NS had collected some observational data (described below) as part of their
pilot study.
In the course of trying to interpret the data, NS told PS about her data and
the problem NS was interested in addressing.
After investigating the problem further, PS emailed JM and KO a one page
proposal for a stratified permutation test for multi-rater inter-rater
reliability.
We (JM, KO, PS) had recently begun developing a general purpose Python package
for permutation tests based on our collaborations called
\texttt{permute}\footnote{\url{http://statlab.github.io/permute/}} and PS
suggested this would be an interesting example to use.

After coming to an initial understanding of NS's underlying research problem
and making certain we (JM, KO, PS) understood how her data was collected, we
cleaned the data, developed a nonparametric approach to assessing IRR
appropriate to the experiment, implemented the approach in Python, incorporated
the resulting code into our evolving Python package of permutation tests,
applied the approach to the cleaned data, documented the code and the analysis,
and wrote up the results in a \LaTeX document. 

We distinguish the following aspects of our project:
(1) understand problem,
(2) get and clean data,
(3) design algorithm,
(4) implement algorithm,
(5) analyze data, and
(6) understand result.
In Figure~\ref{fig:work_process}, we have diagrammed how each aspect of the
project influenced the other aspects as well as an estimate of how much time
we collectively spent on each aspect of the project.
Our estimates are based on the total person hours we spent on each aspect of
the problem.
For example, if all three of us (JM, KO, PS) spent an hour discussing the
problem in a meeting, then that hour involved 3 people hours.
Below we describe the work involved in each aspect of the project.
We have listed the total number of people hours spent on each aspect in the
headings for each subsection.
\jarrod{Need to explain why we are using names and time estimates since
we were asked to remove them.
- give people an idea how ``inexpensive'' (or expensive) working more
  reproducibly is.
- capture how group understanding evolved
- hope that it might be instructive for students and collaborators}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.8\textwidth]{_fig/work_process.png}
  \caption{
  \small
    Each box corresponds to one aspect of our project.
    The percents are estimates of the percent of our time spent on each aspect.
    Edges represent influence.
    \texttt{Understand problem}~$\to$~\texttt{Design algorithm}, for instance,
    corresponds to the fact that we had to understand the problem in order to
    design the correct statistical test.\label{fig:work_process}}
\end{figure}

Since we view computational reproducibility as a cross-cutting concern of all
project aspects, we have adopted a set of computational practices, which we
(JM, KO, PS) followed (almost) whenever we were working on the
project.\footnote{Exceptions include that we did not record all of our
in-person discussions or whiteboard work.  However, we endeavored to
record summaries of these activities.}
These computational practices are described in \citet{millman2014developing}
and are used widely in the open source scientific Python community.
While developed for managing software contributions, these practices are ideal
for ensuring computational reproducibility in scientific and statistical
research.
We will illustrate how we leverage the software infrastructure and development
practices of \texttt{permute} to conduct reproducible and collaborative applied
statistics research with our colleagues.
We discuss the software tools and practices briefly in \S~\ref{key-tools} below.

\subsection{Understand problem (80 hours)}

\philip{
NS was interested in:  What happens in therapy session with autistic kids?
In particular, NS had data and wanted to know whether humans can reliably rate
therapist-patient interactions?
conversations between NS and (primarily) PS to understand the experimental
set-up.
%Time: approximately 10 1-hour meetings
}

Once PS had an initial understanding of NS's problem, we (JM, KO, PS) met
regularly (approximately weekly, sometimes more) as a team to discuss the
project.
Initially these discussions involved a lot of work on whiteboards and asking a
lot of probing and open-ended questions.
This helped us make sure we all gained a shared understanding of the problem.
It also meant that we could improve our understanding by explaining it to one
another as well as asking hard questions about our planned approach and whether
it could address the question of interest.
As our understanding of the problem progressed, our work transitioned from
working on whiteboards to testing our ideas out on a computer.
Often we often used pair programming at this stage and sometimes all three of
us (JM, KO, PS) sat in front of one computer, while one of us would type code
in an interactive IPython session.
This helped ensure that we all understood the problem well and it also helped
us catch errors (typos as well as conceptual errors).
%Time: approximately 10 2-hour meetings.

\subsection{Get and clean data (13 hours)}

The data were collected by NS.
They comprise ratings of segments of 8 videos by 10 trained raters.
Each video is divided into approximately 40 time segments.
In each time segment, none, any, or all of 183 types of activity might be
taking place.
The raters indicated which of those activities was taking place during each
segment of each video.

\philip{
receipt of preliminary data as an Excel spreadsheet; understanding the ``data
dictionary''; vetting for obvious errors (PS);
%Time: approximately 1 hour
several ``trips to the well'' before getting a version of the data that did not
have obvious errors (PS);
%Time: approximately 4 hours
export the Excel data to .csv format (PS);
%Time: approximately 5 minutes.
data anonymization: substitute unique numerical identifiers for raters' names. 
This step was performed using regular expressions in an interactive text
editor, on the .csv file.
It was not performed reproducibly (i.e., not scripted), but it can be checked
readily. (PS) 
%Time: approximately 1 hour.
}

After PS generated the original anonymized data, we committed it to our
repository and added a data loader with tests to ensure that if the data
changed we would know.
At this point, we (JM, PS) screened the anonymized data for transcription
errors (e.g., typos, duplications).
This involved writing a number of EDA tools, which are now included in
\texttt{permute}, to detect (for example) duplicate consecutive rows.
%Time: approximately 5 hours
Once we identified entries incompatible with our understanding of what should
be in the data, JM wrote a \texttt{sed} script to ``correct'' the data for
duplicated entries and inferred typos.
%Time: included above
The exact commands used to clean the data are included in the commit
corresponding to that cleaning step.
After carefully examining the data for potential errors and documenting
every change we made and why, we sent the cleaned data and an explanation
of what we did to NS to verify that the corrections were appropriate.
%Time: 1 hour
As a result, we provide the cleaned data in our project repository as well
as a careful account of its provenance.

\subsection{Design algorithm (25 hours)}

Although we eventually implemented something close to the original test
proposed by PS at the start of the project, we spent significant time focused
on  ``problem appreciation.''
These conversations culminated in our (JM, KO, PS) decision to assess the
reliability one category at a time.
%Time: approximately 4h.
We also decided it was best to treat the videos separately.
A literature search for extant approaches to assessing IRR led us to conclude
that there was no existing suitable method, in part because the experiment was
stratified and in part because standard methods make indefensible parametric
assumptions, which we hoped to avoid.
%Time: 3h
After deciding to use permutation tests, we (JM, KO, PS) then determined what
the appropriate permutation would be: permuting each rater's ratings within a
video, independently across raters and across videos.
We chose to use concordance of ratings as our test statistic within each
stratum.
Finally, we (JM, PS) derived a simple expression for efficiently computing the
concordance.
%Time: 1h
To combine tests across strata, we (JM, KO, PS) decided to use the nonparametric
combination (NPC) of tests.
%Time: 1h
Finally, we (JM, KO, PS) developed a computationally efficient approach to
finding the overall $p$-value for NPC.
%Time: 1.5h

\subsection{Implement algorithm (5 hours)}

\jarrod{I plan to go over this:  https://github.com/statlab/permute/issues/69.
I will write this up as a little report separate from this case study.  I will
also look into how long it takes to run, but I am thinking I might make a
separate repository just for this.  I suspect it will take some time to run and
our tests and doc generation are already costly.  I am also reluctant to put
something we plan to freeze in a project that should evolve.  I am imagining
something that depends on the exact release of permute that we used and has a
permanent web address.}

\jarrod{Once I finish I need to discuss this with Kellie, since she did most of
the hard work as I recall.}
  

\subsection{Analyze data (1 hour)}

By the time we got to this stage, the analysis of the cleaned data was only
about 70 lines of Python, of which 56 are code (the rest are comments).
This includes looping over the 183 categories of activity.

\jarrod{Once I turn the analysis into a stand-alone frozen project, I
will report our results here.}

\subsection{Understand result (20 hours)}

\philip{You will probably want to wait until I have written up the results to
work on this; but, eventually we need a discussion of our final thoughts about
the data and how it did or did not address the question.  Maybe a sentence or
two saying what we learned...what we do differently during data collection next
time.  Understanding result can lead to 1) designing new experiment, 2)
collecting new data, 3) performing different analysis, 4) writing manuscript,
5) something else.  Need to connect this back to the our understanding of the
problem.  We should also say that despite not pursuing this further, it did
yield a new permutation test that could be used in other contexts and which is
available in \texttt{permute}.  }

\section{Pain points}\label{pain-points}

Given our different backgrounds and experiences we (JM, KO, PS) each found
different points in the process challenging. 
However, for all of us the most challenging aspect was the necessary
struggle to understand the fundamental problem and what we could learn
from the data about that problem.

For KO and PS there was a learning curve to master the tools and practices.
This involved understanding the data model used by Git, acquiring habits such
as writing tests for all functions and following a common style guide, as well
as learning to contribute to the project repository indirectly through GitHub's
pull request mechanism.
While learning how to use these tools and acquiring these habits takes effort
and persistence, once mastered the difficulties disappear and the benefits
accrue.

JM was already familiar with the tools and practices.
For JM the most painful part of the project was vetting hand-entered data
to ensure the data were error-free.
Not only was this a laborious process, but it involved inferring what the data
should have been without any direct way to ensure that these inferences were
correct.
The solution to this pain point is to automate the data collection process as
much as possible.
However, when data has already been entered by hand, there is not much that can
be done other than being cautious when ``fixing'' data entry errors and recording
every aspect of the data cleaning process.

\section{Key benefits}\label{key-benefits}

Since \citet{buckheit1995wavelab} popularized the idea of computational
reproducibility, applied statisticians have taken the ideas of version control
and process automation seriously.
At Berkeley, many faculty and researchers in the Department of Statistics and
in the Division of Biostatistics have made the idea of computational
reproducibility central in both their classroom as well as their own work.
Several faculty require that anyone working with them use version
control.\footnote{For example, see:
\url{http://www.stat.berkeley.edu/~epurdom/studentResources.html}}

However, the practices we have adopted go beyond the standard operating
practice of our colleagues.
In particular, our adherence to continuous integration and code review
(see below) is unusual among our colleagues.
We believe that our colleagues would have more reliable code and would be more
confident in their ability to modify their existing code without breaking
downstream dependencies by adopting these practices.

In particular, we have found that our practices provide the following benefits:
(1) it is easy to modify the analysis when errors are found, to apply the
    analysis to new data sets, and so on;
(2) the process is self-documenting, making it easier to draft a paper about
    the results; and
(3) the methods are abstracted from the analysis and incorporated into a
    package so that others can discover, check, use, and extend our methods.

\section{Key tools and practices}\label{key-tools}

As part of the development of our software package \texttt{permute}, we
invested significant effort in setting up a development infrastructure to
ensure our work is tracked, thoroughly and continually tested, and
incrementally improved and documented.
To this end, we have adopted best practices for software development used by
many successful open source projects \citep{millman2014developing}.

\subsection{\label{sec:vc}Version control and code review}

We (JM, KO, PS) use Git\footnote{\url{http://git-scm.com}} as our version
control system (VCS) and GitHub\footnote{\url{https://github.com}} as the
public hosting service for our official \texttt{upstream} repository
(\url{https://github.com/statlab/permute}).
Each of us has our own copy, or fork, of the \texttt{upstream} repository.
We each work on our own repositories and use the \texttt{upstream} repository
as our coordination or integration repository.

This allows us to track and manage how our code changes over time as well as
review all new functionality before merging it into the \texttt{upstream}
repository.
To get new code or text integrated in the \texttt{upstream} repository, we use
GitHub's \emph{pull request} mechanism.
This enables us to review code and text before integrating it.
Below, we describe how we automate testing our code to generate reports for all
pull requests.
This way we can reduce the risk that changes to our code break existing
functionality.
Once a pull request is reviewed and accepted, it is merged into the
\texttt{upstream} repository.

Requiring all new code to undergo review provides several benefits.
Code review increases the quality and consistency of our code.
It helps maintain a high level of test coverage (see below).
Moreover, it also helps keep the development team aware of the work other team
members are doing.
While we are currently a small team and we meet regularly, having the code
review system in place will make it easier for new people to contribute as well
as capturing our design discussions and decisions for future reference.

\subsection{\label{sec:test}Testing and continuous integration}

We use the \texttt{nose} testing framework for automating our testing
procedures.\footnote{\url{https://nose.readthedocs.org}}
This is the standard testing framework\footnote{As \texttt{nose} has been in
maintenance mode for the last few years, projects are starting to look
into newer testing frameworks.
We are migrating to \texttt{pytest} (\url{http://pytest.org}).
While we may transition to a new testing framework, any system we
migrate to will have the same basic features and benefits as \texttt{nose}.}
used by the core packages in the scientific Python ecosystem.
Automating the tests allows us to monitor a proxy for code correctness when
making changes as well as simplifying the code review process for new code.
Without automated testing, we would have to manually test all the code every
time a change is proposed.
The \texttt{nose} testing framework simplifies test creation, discovery, and
running.
It has an extensive set of plugins to add functionality for coverage reporting,
test annotation, profiling, as well as inspecting and testing documentation.

\begin{figure}
  \begin{centering}
    \includegraphics[width=\textwidth]{_fig/pull-request-ci.png}\par
  \end{centering}

  \caption{\label{fig:pull-request}
  \small
    Each pull request triggers an automated system to run the full test suite on
    the updated code.
    This means that when you go to review a pull request you can immediately see
    whether the change breaks any of the tests as well as whether the new
    code decreases the overall test coverage.
    For example, the above report indicates that the associated pull request does not
    break existing code and does not change our test coverage.}
\end{figure}

Our goal is to test every line of code.
For example, not only do we want to test every function in our package, but if
a specific function has internal logic we want to test each possible execution
path through the function.
Having tested each line of code increases our confidence in our code, but
more importantly provides us some measure of assurance that changes we make do
not break existing code.
It also increases our confidence that new code works, which reduces the
friction of accepting contributions.
Currently over 98\% of \texttt{permute}'s lines of code get executed at least
once by our test system.

We have configured Travis CI\footnote{\url{https://travis-ci.org}} and
\texttt{coveralls}\footnote{\url{https://coveralls.io}} to be automatically
triggered whenever a commit is made to a pull request or the upstream master
(see Figure~\ref{fig:pull-request}).
These systems then run the full test suite  using different versions of our
dependencies (e.g., Python 2.7 and 3.4) every time a new commit is made to a
repository or pull request.
\jarrod{Explain Travis CI and coveralls}

\subsection{\label{sec:doc}Documentation}

We use Sphinx\footnote{\url{http://sphinx-doc.org}} as our documentation system
and already have good developer documentation and the foundation for
high-quality user documentation.
Sphinx is the standard documentation system for Python projects and is used by
the core scientific Python packages.
We use Python docstrings and follow the NumPy docstring
standard\footnote{\url{https://github.com/numpy/numpy/blob/master/doc/HOWTO\_DOCUMENT.rst.txt}}
to document all the modules and functions in \texttt{permute}.
Using Sphinx and some NumPy extensions, we have a system for autogenerating the
project documentation (as HTML or PDF) using the docstrings as well as
stand-alone text written in a light-weight markdown-like language, called
reStructuredText\footnote{\url{http://docutils.sourceforge.net/rst.html}}.
This system enables us to easily embed references, figures, code that is
auto-run during documentation generation, as well as mathematics using \LaTeX.

\subsection{\label{sec:release}Release management}

Our development workflow ensures that the official \texttt{upstream} repository
is always stable and ready for use.
This means anyone can get our official upstream master at any point, install it
and start using it.
We also make official releases available as source tarballs as well as Python
built-packages\footnote{Presently our code is pure Python, but we release
Python wheels.
Wheels are the new standard built-package format for Python.} uploaded to the
Python Package Index, or PyPI,\footnote{PyPI is the Python equivalent of \emph{The
Comprehensive R Archive Network} (CRAN).} with release announcements posted to
our mailing list.

To install the latest release of \texttt{permute} and its dependencies, type
the following command from a shell prompt (assuming you have Python and a
recent version of pip):
\begin{verbatim}
$ pip install permute==0.1a2
\end{verbatim}
\jarrod{Explain virtualenv and why releasing allows us to install the
old dependency as needed.
Maybe something about how the process of developing a general tool and a
specific analysis work together in tandem.}

\bibliographystyle{plainnat}
\bibliography{millman_ottoboni_stark}

\pagebreak

\section*{Appendix}

\subsection*{A stratified permutation test for multi-rater inter-rater reliability.}

There are $S$ strata.
There are $N_s$ items in stratum $s$.
There are $N = \sum_{s=1}^S N_s$ items in all.

There are $C$ non-exclusive categories to which each of the $N$ items might
belong; an item might belong to none of the categories.
That is, each item might be ``labeled'' with any of the $2^C$ subsets
of the $C$ labels, including the empty set.

There are $R$ ``raters,'' each of whom labels each of the $N$ items with zero
or more elements of $C$.

Define $L_{s,i,c,r} = 1$, if rater $r$ assigns label $c$ to item $i$ in stratum
$s$ and $L_{s,i,c,r} = 0$ if not.

We observe $\{ L_{s,i,c,r} \}$ for $s=1, \dots, S$;  $i=1, \dots, N_s$;
$c=1, \dots, C$; and $r=1, \dots, R$.

We want to know whether the categorizations are ``reliable,'' in the sense that
agreement among the raters is higher than would be expected ``by chance.''
The reliability of each category $c$ is of interest, rather than an overall
rating for all $C$ categories.

Fix $c$, since we are considering only one category at a time.

The null hypothesis for category $c$ is that, for each rater $r$, and each
stratum $s$, the values $\{ L_{s,i,c,r} \}$ are exchangeable; that for each
rater $r$, the values $\{ L_{s,i,c,r} \}$ for different strata $s$ are
independent; and that the values are independent across raters.

Our test conditions on the sets of labels each rater assigns within each
stratum, but not on the items to which those labels are assigned.
The null distribution involves permuting the assignments each given rater makes
of category $c$ to items within each stratum $s$, permuting independently
across raters and across strata.

The test statistic within stratum $s$ is
\begin{align*}
\rho_s &\equiv \frac{1}{N_s {R \choose 2}} \sum_{i=1}^{N_s}
              \sum_{r=1}^{R-1} \sum_{v=r+1}^R 1(L_{s,i,r} = L_{s,i,v}) \\
              &= \frac{1}{N_s R(R-1)} \sum_{i=1}^{N_s}
                (y_{si}(y_{si}-1) + (R-y_{si})(R-y_{si}-1)).
\end{align*}

That is, within each stratum, we count the number of concordant pairs of
assignments.
If all $R$ raters agree whether item $i$ in stratum $s$ belongs to category
$c$, that contributes a term ${R \choose 2}$ to the sum.
If only half agree, the term for item $i$ contributes $2 {N/2 \choose 2}$ to
the sum.
The normalization makes perfect agreement within stratum $s$ correspond to
$\rho_s = 1$.

To combine the results across strata to get an overall $p$-value, we could
use any of the methods we have discussed, or the NPC (nonparametric
combination of test) methods described in Pesarin and Salmaso, based on
the $p$-values in different strata.
For instance, Fisher's combination statistic is
\begin{align*}
\lambda &= - \sum_{s=1}^S w_s \log \hat{p}_s,
\end{align*}
where the nonnegative weights $\{w_s\}$ are chosen in some sensible manner
(e.g., $w_s = N_s^{-1/2}$ would be reasonable).

\end{document}
